{
  "model_id": "meta-llama/Llama-3.1-8B-Instruct",
  "local_path": "~/models/Llama-3.1-8B-Instruct",
  "deployment_type": "vllm",
  "num_gpus": 1,
  "dtype": "auto",
  "max_model_len": null,
  "generation_params": {
    "max_tokens": 4096,
    "temperature": 0.7,
    "top_p": 0.9
  }
} 